import streamlit as st

st.set_page_config(page_title="AI for Climate Adaptation ‚Äì Evaluation", layout="wide")

st.title("AI for Climate Adaptation ‚Äì Evaluation Platform")

st.markdown("""
This platform is part of a **research project on the effectiveness of large language models (LLMs)** in supporting climate change adaptation.

The objective is to systematically evaluate how different AI systems perform in generating responses to climate-related questions, with a focus on four key dimensions:

- **Relevance** ‚Äì How well the response addresses the given question
- **Credibility** ‚Äì Scientific accuracy and plausibility
- **Uncertainty communication** ‚Äì Clarity in expressing limitations or confidence levels
- **Actionability** ‚Äì Usefulness of the response for decision-making or planning

---

### üß™ Methodology

Participants are presented with pairs of responses generated by different AI systems and asked to evaluate them according to the criteria above.

The responses are anonymized to ensure unbiased comparison across models.

---

### üìÇ How to Participate

#### 1Ô∏è‚É£ **Registration and Access**

To start evaluating:

1. **Go to "Account Management"** page via the sidebar
2. **Select "Register"** to create a new account or "Login" if you're already registered
3. **Fill in the required information**:
   - Username (will be your unique identifier)
   - Secure password
   - Professional background (e.g., researcher, student, professional)
   - Specific role (e.g., climatologist, engineer, policy maker)
   - Preference for receiving updates on study results

#### 2Ô∏è‚É£ **Evaluation Process**

Once logged in:

1. **Access the "Evaluation" page** from the sidebar
2. **You will receive a pair of responses** generated by different AI systems for the same climate question
3. **Evaluate each response** on a scale from 1 to 5 for each criterion:
   - **Relevance**: How well the response addresses the given question
   - **Credibility**: Scientific accuracy and plausibility of the information
   - **Uncertainty communication**: Clarity in expressing limitations or confidence levels
   - **Actionability**: Usefulness of the response for decision-making or planning

4. **Submit your evaluation** and proceed to the next pair
5. **Continue as long as you wish** - there's no minimum or maximum number of evaluations required

#### üìã **Tips for Effective Evaluation**

- **Read carefully** both responses before evaluating
- **Consider the context** of the climate question asked
- **Be objective** - responses don't show which AI generated them
- **Evaluate each criterion independently** - a response can be relevant but not credible
- **Take your time** - there's no rush for accurate evaluation

---

""")
