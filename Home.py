import streamlit as st

st.set_page_config(page_title="AI for Climate Adaptation â€“ Evaluation", layout="wide")

st.title("AI for Climate Adaptation â€“ Evaluation Platform")

st.markdown("""
This platform is part of a **research project on the effectiveness of large language models (LLMs)** in supporting climate change adaptation.

The objective is to systematically evaluate how different AI systems perform in generating responses to climate-related questions, with a focus on four key dimensions:

- **Relevance** â€“ How well the response addresses the given question
- **Credibility** â€“ Scientific accuracy and plausibility
- **Uncertainty communication** â€“ Clarity in expressing limitations or confidence levels
- **Actionability** â€“ Usefulness of the response for decision-making or planning

---

### ğŸ§ª Methodology

Participants are presented with pairs of responses generated by different AI systems and asked to evaluate them according to the criteria above.

The responses are anonymized to ensure unbiased comparison across models.

---

### ğŸ“‚ How to Participate

#### ğŸš€ **Quick Start**

To start evaluating immediately:

1. **Go directly to the "Evaluation" page** via the sidebar
2. **Enter your username** when prompted
3. **If you're a new user**, you'll be asked to provide some background information:
   - Professional background (e.g., research field, interest area)
   - Current role (e.g., researcher, student, professional)
   - Institution/Organization (optional)
4. **Start evaluating!** You'll receive pairs of AI-generated responses to evaluate

#### ğŸ“ **Evaluation Process**

Once you enter the evaluation page:

1. **You will receive a pair of responses** generated by different AI systems for the same climate question
2. **Evaluate each response** on a scale from 1 to 10 for each criterion:
   - **Relevance**: How well the response addresses the given question
   - **Credibility**: Scientific accuracy and plausibility of the information
   - **Uncertainty communication**: Clarity in expressing limitations or confidence levels
   - **Actionability**: Usefulness of the response for decision-making or planning

3. **Submit your evaluation** and proceed to the next pair
4. **Continue as long as you wish** - there's no minimum or maximum number of evaluations required

#### ğŸ“‹ **Tips for Effective Evaluation**

- **Read carefully** both responses before evaluating
- **Consider the context** of the climate question asked
- **Be objective** - responses don't show which AI generated them
- **Evaluate each criterion independently** - a response can be relevant but not credible
- **Take your time** - there's no rush for accurate evaluation

---

""")
